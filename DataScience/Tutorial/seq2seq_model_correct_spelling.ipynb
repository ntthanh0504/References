{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 2126280\n",
      "First 5 train: ['Bộ phim lần đầu được công chiếu tại liên hoan phim Rome 2007 và sau đó được chiếu ở Fairbanks, Alaska ngày 21 tháng 9 năm 2007.', 'Những kiểu áo sơ mi may theo chất liệu cotton, KT, hay có chút co giãn năm nay cũng được các bạn trẻ ưa chuộng.', 'Đương kim tổng thống là Andrés Manuel López Obrador, người nhậm chức vào ngày 1 tháng 12 năm 2018.', 'Centaurea gloriosa là một loài thực vật có hoa trong họ Cúc.', 'Sau này mới thấy người ta nói, đó là con rắn đực đi tìm người ăn thịt để trả thù cho rắn cái, bà T cho biết thêm.']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_zip(zip_file, extract_dir):\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "def data_train(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        train = f.readlines()\n",
    "    train = [x.strip() for x in train]\n",
    "    return train\n",
    "\n",
    "train = data_train('vietnamese_tone_prediction\\\\train.txt')\n",
    "print('Length of train:', len(train))\n",
    "print('First 5 train:', train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanh\\AppData\\Local\\Temp\\ipykernel_11676\\2709821622.py:24: FutureWarning: Possible nested set at position 2\n",
      "  s = re.sub(r, r\" \\1 \", s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vui vẻ , hòa đồng , hoạt bát'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def remove_tone_line(str):\n",
    "    intab_l = \"áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ\"\n",
    "    intab_u = \"ÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\"\n",
    "    intab = intab_l + intab_u\n",
    "    \n",
    "    outtab_l = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"\n",
    "    outtab_u = \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"\n",
    "    outtab = outtab_l + outtab_u\n",
    "    \n",
    "    r = re.compile(\"|\".join(intab))\n",
    "    replaces_dict = dict(zip(intab, outtab))\n",
    "    return r.sub(lambda m: replaces_dict[m.group(0)], str)\n",
    "\n",
    "remove_tone_line('Đi một ngày đàng học 1 sàng khôn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    marks = '[.!?,-${}()]'\n",
    "    r = \"([\"+\"\\\\\".join(marks)+\"])\"\n",
    "    s = re.sub(r, r\" \\1 \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "normalizeString('vui vẻ, hòa đồng, hoạt bát')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 train: ['Bộ phim lần đầu được công chiếu tại liên hoan phim Rome 2007 và sau đó được chiếu ở Fairbanks , Alaska ngày 21 tháng 9 năm 2007 .', 'Những kiểu áo sơ mi may theo chất liệu cotton , KT , hay có chút co giãn năm nay cũng được các bạn trẻ ưa chuộng .', 'Đương kim tổng thống là Andrés Manuel López Obrador , người nhậm chức vào ngày 1 tháng 12 năm 2018 .', 'Centaurea gloriosa là một loài thực vật có hoa trong họ Cúc .', 'Sau này mới thấy người ta nói , đó là con rắn đực đi tìm người ăn thịt để trả thù cho rắn cái , bà T cho biết thêm .']\n",
      "First 5 train_no_tone: ['Bo phim lan dau duoc cong chieu tai lien hoan phim Rome 2007 va sau do duoc chieu o Fairbanks , Alaska ngay 21 thang 9 nam 2007 .', 'Nhung kieu ao so mi may theo chat lieu cotton , KT , hay co chut co gian nam nay cung duoc cac ban tre ua chuong .', 'Duong kim tong thong la Andres Manuel Lopez Obrador , nguoi nham chuc vao ngay 1 thang 12 nam 2018 .', 'Centaurea gloriosa la mot loai thuc vat co hoa trong ho Cuc .', 'Sau nay moi thay nguoi ta noi , do la con ran duc di tim nguoi an thit de tra thu cho ran cai , ba T cho biet them .']\n"
     ]
    }
   ],
   "source": [
    "train = [normalizeString(x) for x in train]\n",
    "train_no_tone = [remove_tone_line(x) for x in train]\n",
    "print('First 5 train:', train[:5])\n",
    "print('First 5 train_no_tone:', train_no_tone[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Trim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0 # Used for padding short sentences\n",
    "SOS_token = 1 # Start-of-sentence token\n",
    "EOS_token = 2 # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word): \n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        \n",
    "        print('keep_words {} / {} = {:.4f}'.format(len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))\n",
    "        \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vui vẻ hòa đồng', 'vẻ hòa đồng hoạt', 'hòa đồng hoạt bát']\n",
      "['vui vẻ hòa PAD PAD']\n"
     ]
    }
   ],
   "source": [
    "def ngram(text, length=4):\n",
    "    words = text.split(' ')\n",
    "    ngrams = []\n",
    "    if len(words) <= length:\n",
    "        words = words + ['PAD']*(length-len(words))\n",
    "        return [' '.join(words)]\n",
    "    else:\n",
    "        for i in range(len(words)-length+1):\n",
    "            ngrams.append(' '.join(words[i:i+length]))\n",
    "        return ngrams\n",
    "\n",
    "print(ngram('vui vẻ hòa đồng hoạt bát', 4))\n",
    "print(ngram('vui vẻ hòa', 5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "train_ngram = list(itertools.chain.from_iterable([ngram(x) for x in train]))\n",
    "train_no_tone_ngram = list(itertools.chain.from_iterable([ngram(x) for x in train_no_tone]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 corpus: [('Bộ phim lần đầu', 'Bo phim lan dau'), ('phim lần đầu được', 'phim lan dau duoc'), ('lần đầu được công', 'lan dau duoc cong'), ('đầu được công chiếu', 'dau duoc cong chieu'), ('được công chiếu tại', 'duoc cong chieu tai')]\n"
     ]
    }
   ],
   "source": [
    "corpus = list(zip(train_ngram, train_no_tone_ngram))\n",
    "print('First 5 corpus:', corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "MAX_LENGTH = 4  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    # Tách dấu câu nếu kí tự liền nhau\n",
    "    s = re.sub(r\"([.!?,\\-\\&\\(\\)\\[\\]])\", r\" \\1 \", s)\n",
    "    # Thay thế nhiều spaces bằng 1 space.\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(lines, corpus_name = 'corpus'):\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(str(s)) for s in l] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "voc, pairs = readVocs(corpus)  \n",
    "  \n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# # Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(voc, pairs):\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    # pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "  \n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(voc, pairs)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
